{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "tags": []
   },
   "source": [
    "# Clustering analysis use-case with Synthetic FHIR and VCF Data\n",
    "### Overview\n",
    "**Important:** This walk-through assumes you have already used [our previous notebook](https://github.com/microsoft/genomicsnotebook/blob/main/sample-notebooks/fhir_long_read.ipynb)[<sup>[1]</sup>](#r1) to download FHIR data[<sup>[3]</sup>](#r3) in Parquet format[<sup>[4]</sup>](#r4).  \n",
    "A Microsoft Healthcare and Life Sciences blog post accompanies the previous notebook, and can be found [here](https://techcommunity.microsoft.com/t5/healthcare-and-life-sciences/convert-synthetic-fhir-and-pacbio-vcf-data-to-parquet-and/ba-p/3577038)[<sup>[2]</sup>](#r2).\n",
    "\n",
    "In **Section 1**, FHIR data stored in Parquet format is parsed into a pandas DataFrame[<sup>[5]</sup>](#r5) for further analysis.  \n",
    "In **Section 2**, individual patient VCFs are merged into a single joint VCF. We are starting from the raw data again, rather than the Parquet data, in order to combine the VCFs in a different format (a joint VCF).  \n",
    "In **Section 3**, the joint VCF is converted into a pandas DataFrame and missing values are imputed. Since our input is [VCF and not GVCF](https://github.com/broadinstitute/gatk-docs/blob/master/gatk3-faqs/What_is_a_GVCF_and_how_is_it_different_from_a_'regular'_VCF%3F.md)[<sup>[6]</sup>](#r6), absent variant calls are assumed to all be homozygous reference calls of average confidence. In future work, we plan to work with GVCF inputs.  \n",
    "In **Section 4**, the two DataFrames are merged.  \n",
    "In **Section 5**, we perform per-patient clustering analysis.\n",
    "\n",
    "### Contents\n",
    "[1. Extract FHIR Data: from .parquet to DataFrame](#1)  \n",
    "[2. Merge individual VCFs into a joint VCF](#2)  \n",
    "[3. Extract joint VCF: from .vcf to DataFrame](#3)  \n",
    "[4. Data Merging and Normalization](#4)  \n",
    "[5. Clustering Analysis](#5)  \n",
    "[References](#references)\n",
    "\n",
    "Note that we are providing an example architectural design to illustrate how Microsoft tools can be utilized to connect the pieces together (data + interoperability + secure cloud + AI tools), enabling researchers to conduct research analyzing genomics+clinical data. We are not providing or recommending specific instructions for how investigators should conduct their research with this notebook – we will leave that to the professionals!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Extract FHIR Data\n",
    "### from `.parquet` to `DataFrame` <a id=\"1\"></a>\n",
    "First, we'll import the necessary libraries and set some useful variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manage imports\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from datetime import datetime\n",
    "import os, json, subprocess\n",
    "\n",
    "# filepaths\n",
    "fhir_dir = \"*/fhir/result\"\n",
    "date = \"2022/07/26\"\n",
    "pb_dir = \"*/pacbio\"\n",
    "\n",
    "# globals\n",
    "max_patients = 50\n",
    "max_variants = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll parse all the FHIR `Patient` data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse patient information from each .parquet file\n",
    "fhir_dfs = []\n",
    "for patient_file in glob(f\"{fhir_dir}/Patient/{date}/**/*.parquet\"):\n",
    "    \n",
    "    # create dataframe for this .parquet file\n",
    "    fhir_df = pd.DataFrame()\n",
    "    orig_fhir_df = pd.read_parquet(patient_file)\n",
    "\n",
    "    # set basic fields\n",
    "    fhir_df[\"name\"]    = [f\"{' '.join(name[0]['prefix']) if name[0]['prefix'] else ''} {' '.join(name[0]['given'])} {name[0]['family']}\" for name in orig_fhir_df[\"name\"]]\n",
    "    fhir_df[\"id\"]      = orig_fhir_df[\"id\"]\n",
    "    fhir_df[\"city\"]    = [addr[0][\"city\"]    for addr in orig_fhir_df[\"address\"]]\n",
    "    fhir_df[\"state\"]   = [addr[0][\"state\"]   for addr in orig_fhir_df[\"address\"]]\n",
    "    fhir_df[\"country\"] = [addr[0][\"country\"] for addr in orig_fhir_df[\"address\"]]\n",
    "    fhir_df[\"gender\"]  = orig_fhir_df[\"gender\"]\n",
    "    fhir_df[\"married\"] = [married[\"text\"][0] for married in orig_fhir_df[\"maritalStatus\"]]\n",
    "    fhir_df[\"dead\"]    = [dead is not None for dead in orig_fhir_df[\"deceased\"]]\n",
    "    fhir_df[\"twin+\"]   = [mult[\"boolean\"] if mult[\"boolean\"] is not None else True for mult in orig_fhir_df[\"multipleBirth\"]]\n",
    "    \n",
    "    # set age in years from birthDate\n",
    "    ages = []\n",
    "    for birthdate in orig_fhir_df[\"birthDate\"]:\n",
    "        year, month, day = map(int, birthdate.split(\"-\"))\n",
    "        ages.append((datetime.now()-datetime(year, month, day)).total_seconds() / (60*60*24*365.25))\n",
    "    fhir_df[\"age\"]     = ages\n",
    "\n",
    "    # set \"medical record number\", from a host of other identifiers (social security, etc.)\n",
    "    mrns = []\n",
    "    for identifier_list in orig_fhir_df[\"identifier\"]:\n",
    "        for ident in identifier_list:\n",
    "            if ident['type'] and json.loads(ident['type']['coding'])[0]['code'] == \"MR\":\n",
    "                mrns.append(ident['value'])\n",
    "    fhir_df[\"mrn\"]     = mrns\n",
    "    \n",
    "# merge information\n",
    "    fhir_dfs.append(fhir_df)\n",
    "all_fhir_df = pd.concat(fhir_dfs).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each patient, let's add their newest and oldest medication information based on their FHIR `MedicationRequest` history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse medication information from each .parquet file\n",
    "meds_dfs = []\n",
    "for meds_file in glob(f\"{fhir_dir}/MedicationRequest/{date}/**/*.parquet\"):\n",
    "    \n",
    "    # create a dataframe for this .parquet file\n",
    "    meds_df = pd.DataFrame()\n",
    "    orig_meds_df = pd.read_parquet(meds_file)\n",
    "    \n",
    "    # add simple fields\n",
    "    meds_df[\"id\"] = orig_meds_df[\"id\"]\n",
    "    meds_df[\"code\"] = [\"None\" if med[\"codeableConcept\"] is None else med[\"codeableConcept\"][\"coding\"][0][\"code\"] for med in orig_meds_df[\"medication\"]]\n",
    "    meds_df[\"type\"] = [\"None\" if med[\"codeableConcept\"] is None else med[\"codeableConcept\"][\"text\"] for med in orig_meds_df[\"medication\"]]\n",
    "    meds_df[\"mrn\"] = [subject[\"reference\"].split(':')[2] for subject in orig_meds_df[\"subject\"]] # urn:uuid:<mrn>\n",
    "    \n",
    "    # calculate years elapsed since medication was first requested\n",
    "    elapsed = []\n",
    "    for issued in orig_meds_df[\"authoredOn\"]:\n",
    "        year, month, day = map(int, issued.split(\"T\")[0].split(\"-\"))\n",
    "        elapsed.append((datetime.now()-datetime(year, month, day)).total_seconds() / (60*60*24*365.25))\n",
    "    meds_df[\"elapsed\"] = elapsed\n",
    "\n",
    "# merge information\n",
    "    meds_dfs.append(meds_df)\n",
    "all_meds_df = pd.concat(meds_dfs).reset_index(drop=True)\n",
    "all_meds_df\n",
    "\n",
    "# get newest meds\n",
    "new_meds_df = all_meds_df.loc[all_meds_df.groupby('mrn')['elapsed'].agg('idxmin')]\n",
    "new_codes, new_elapsed, new_types = [], [], []\n",
    "for mrn in all_fhir_df['mrn']:\n",
    "    try:\n",
    "        new_codes.append(new_meds_df[new_meds_df['mrn'] == mrn].iloc[0]['code'])\n",
    "        new_elapsed.append(new_meds_df[new_meds_df['mrn'] == mrn].iloc[0]['elapsed'])\n",
    "        new_types.append(new_meds_df[new_meds_df['mrn'] == mrn].iloc[0]['type'])\n",
    "    except IndexError:\n",
    "        new_codes.append(0)\n",
    "        new_elapsed.append(0)\n",
    "        new_types.append(\"None\")\n",
    "all_fhir_df['newest_med_code'] = new_codes\n",
    "all_fhir_df['newest_med_elapsed'] = new_elapsed\n",
    "all_fhir_df['newest_med_type'] = new_types\n",
    "\n",
    "# get oldest meds\n",
    "old_meds_df = all_meds_df.loc[all_meds_df.groupby('mrn')['elapsed'].agg('idxmax')]\n",
    "old_codes, old_elapsed, old_types = [], [], []\n",
    "for mrn in all_fhir_df['mrn']:\n",
    "    try:\n",
    "        old_codes.append(old_meds_df[old_meds_df['mrn'] == mrn].iloc[0]['code'])\n",
    "        old_elapsed.append(old_meds_df[old_meds_df['mrn'] == mrn].iloc[0]['elapsed'])\n",
    "        old_types.append(old_meds_df[old_meds_df['mrn'] == mrn].iloc[0]['type'])\n",
    "    except IndexError:\n",
    "        old_codes.append(0)\n",
    "        old_elapsed.append(0)\n",
    "        old_types.append(\"None\")\n",
    "all_fhir_df['oldest_med_code'] = old_codes\n",
    "all_fhir_df['oldest_med_elapsed'] = old_elapsed\n",
    "all_fhir_df['oldest_med_type'] = old_types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also add their newest and oldest diagnoses, from FHIRs `DiagnosticReport` resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse diagnosis report information from each .parquet file\n",
    "reports_dfs = []\n",
    "for reports_file in glob(f\"{fhir_dir}/DiagnosticReport/{date}/**/*.parquet\"):\n",
    "    \n",
    "    # create a dataframe for this .parquet file\n",
    "    reports_df = pd.DataFrame()\n",
    "    orig_reports_df = pd.read_parquet(reports_file)\n",
    "    \n",
    "    # add simple fields\n",
    "    reports_df[\"id\"]      = orig_reports_df[\"id\"]\n",
    "    reports_df[\"code\"]    = [code[\"coding\"][0][\"code\"] for code in orig_reports_df[\"code\"]]\n",
    "    reports_df[\"type\"]    = [code[\"text\"] for code in orig_reports_df[\"code\"]]\n",
    "    reports_df[\"mrn\"]     = [subject[\"reference\"].split(':')[2] for subject in orig_reports_df[\"subject\"]] # urn:uuid:<mrn>\n",
    "    \n",
    "    # calculate years elapsed since report/diagnosis was issued\n",
    "    elapsed = []\n",
    "    for issued in orig_reports_df[\"issued\"]:\n",
    "        year, month, day = map(int, issued.split(\"T\")[0].split(\"-\"))\n",
    "        elapsed.append((datetime.now()-datetime(year, month, day)).total_seconds() / (60*60*24*365.25))\n",
    "    reports_df[\"elapsed\"] = elapsed\n",
    "    \n",
    "# merge information\n",
    "    reports_dfs.append(reports_df)\n",
    "all_reports_df = pd.concat(reports_dfs).reset_index(drop=True)\n",
    "\n",
    "# get newest reports\n",
    "new_reports_df = all_reports_df.loc[all_reports_df.groupby('mrn')['elapsed'].agg('idxmin')]\n",
    "new_reports_df.head(50)\n",
    "new_codes = []\n",
    "for mrn in all_fhir_df['mrn']:\n",
    "    new_codes.append(new_reports_df[new_reports_df['mrn'] == mrn].iloc[0]['code'])\n",
    "all_fhir_df['newest_report_code'] = new_codes\n",
    "new_elapsed = []\n",
    "for mrn in all_fhir_df['mrn']:\n",
    "    new_elapsed.append(new_reports_df[new_reports_df['mrn'] == mrn].iloc[0]['elapsed'])\n",
    "all_fhir_df['newest_report_elapsed'] = new_elapsed\n",
    "new_types = []\n",
    "for mrn in all_fhir_df['mrn']:\n",
    "    new_types.append(new_reports_df[new_reports_df['mrn'] == mrn].iloc[0]['type'])\n",
    "all_fhir_df['newest_report_type'] = new_types\n",
    "\n",
    "# get oldest reports\n",
    "old_reports_df = all_reports_df.loc[all_reports_df.groupby('mrn')['elapsed'].agg('idxmax')]\n",
    "old_codes = []\n",
    "for mrn in all_fhir_df['mrn']:\n",
    "    old_codes.append(old_reports_df[old_reports_df['mrn'] == mrn].iloc[0]['code'])\n",
    "all_fhir_df['oldest_report_code'] = old_codes\n",
    "old_elapsed = []\n",
    "for mrn in all_fhir_df['mrn']:\n",
    "    old_elapsed.append(old_reports_df[old_reports_df['mrn'] == mrn].iloc[0]['elapsed'])\n",
    "all_fhir_df['oldest_report_elapsed'] = old_elapsed\n",
    "old_types = []\n",
    "for mrn in all_fhir_df['mrn']:\n",
    "    old_types.append(old_reports_df[old_reports_df['mrn'] == mrn].iloc[0]['type'])\n",
    "all_fhir_df['oldest_report_type'] = old_types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, let's limit the dataset size to the specified maximum number of patients and preview the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_fhir_df = all_fhir_df.drop(all_fhir_df.index[max_patients:])\n",
    "all_fhir_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Merge individual VCFs into a joint VCF <a id=\"2\"></a>\n",
    "First, we need to install `bcftools` in order to process our VCF files. The `bcftools` documentation[<sup>[7]</sup>](#r7) can be found [here](https://samtools.github.io/bcftools/bcftools.html).  \n",
    "\n",
    "`sudo apt-get install bcftools` is insufficient because the Ubuntu 18.04 package repository contains `bcftools-1.7`, which failed for us on this processing pipeline with an old bug. In the following cell, we install a more recent version, `bcftools-1.15`, from source.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd */code\n",
    "!sudo apt-get -y install liblzma-dev libbz2-dev\n",
    "!wget https://github.com/samtools/bcftools/releases/download/1.15.1/bcftools-1.15.1.tar.bz2\n",
    "!tar -xf bcftools-1.15.1.tar.bz2\n",
    "%cd bcftools-1.15.1\n",
    "!sudo make install\n",
    "!bcftools --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must also install `tabix`[<sup>[8]</sup>](#r8), a tool which indexes VCF files for faster processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo apt-get install -y tabix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're first going to pre-process our VCF files by splitting up sites with multiple alleles into separate records, each with a single allele. For example,\n",
    "\n",
    "CHROM  | POS  | REF  | ALT \n",
    "-------|------|------|----\n",
    "chr20  | 1232 | A    | T,C\n",
    "\n",
    "will become\n",
    "\n",
    "CHROM  | POS  | REF  | ALT  \n",
    "-------|------|------|----\n",
    "chr20  | 1232 | A    | T  \n",
    "chr20  | 1232 | A    | C\n",
    "\n",
    "This ensures that the number of fields is static for each entry, particularly for the `PL` and `AF` columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# skip VCFs which have already been split/pruned\n",
    "orig_vcfs = set(glob(f\"{pb_dir}/**/*.vcf.gz\", recursive=True)) - \\\n",
    "            set(glob(f\"{pb_dir}/**/*_split.vcf.gz\", recursive=True)) - \\\n",
    "            set(glob(f\"{pb_dir}/**/*_pruned.vcf.gz\", recursive=True))\n",
    "\n",
    "for vcf_fn in orig_vcfs:\n",
    "    print(f\"Splitting multi-alleles in '{vcf_fn}'\")\n",
    "    subprocess.run([\n",
    "        \"bcftools\", \"norm\", \n",
    "        \"--multiallelics\", \"-both\",\n",
    "        \"--output-type\", \"z\",\n",
    "        \"--output\", f\"{vcf_fn[:-7]}_split.vcf.gz\", # remove .vcf.gz\n",
    "        vcf_fn\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we perform LD (linkage disequilibrium) pruning[<sup>[9]</sup>](#r9) in order to remove variants with high covariance. Since we're only going to be looking at a small subset of each patient's variants, we want to make sure that the variants we do investigate are fairly independent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for vcf_fn in glob(f\"{pb_dir}/**/*_split.vcf.gz\", recursive=True):\n",
    "    print(f\"Pruning variants in '{vcf_fn}'\")\n",
    "    subprocess.run([\n",
    "        \"bcftools\", \"+prune\", \n",
    "        \"-m\", \"0.2\",\n",
    "        vcf_fn,\n",
    "        \"--output-type\", \"z\",\n",
    "        \"--output\", f\"{vcf_fn[:-13]}_pruned.vcf.gz\", # remove _split.vcf.gz\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Index all individual VCFs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "for vcf_fn in glob(f\"{pb_dir}/**/*_pruned.vcf.gz\", recursive=True): # no gvcfs\n",
    "    print(f\"Indexing '{vcf_fn}'\")\n",
    "    subprocess.run([\"tabix\", \"-f\", \"-p\", \"vcf\", vcf_fn])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge VCF files into single joint VCF, and index it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "subprocess.run([\"bcftools\", \"merge\"] + \n",
    "        [\"-m\", \"none\"] +\n",
    "        list(glob(f\"{pb_dir}/**/*_pruned.vcf.gz\", recursive=True))[:max_patients] +\n",
    "        [\"-o\", f\"{pb_dir}_joint/{max_patients}_patients.vcf.gz\"]\n",
    ")\n",
    "subprocess.run([\"tabix\", \"-f\", \"-p\", \"vcf\", f\"{pb_dir}_joint/{max_patients}_patients.vcf.gz\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Extract joint VCF <a id=\"3\"></a>\n",
    "### from `.vcf` to `DataFrame`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll use `bcftools query` to convert our VCF to a more general format that works well with existing data science libraries: TSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# convert joint VCF to CSV\n",
    "subprocess.run([\"bcftools\", \"query\", \n",
    "        \"--print-header\",\n",
    "        \"-f\", \"%CHROM\\t%POS\\t%TYPE\\t%REF\\t%ALT\\t%QUAL\\t%FILTER\\t%INFO/DP\\t%INFO/AF\\t%INFO/AQ\\t%INFO/AN\\t%INFO/AC[\\t%GT\\t%AD\\t%DP\\t%GQ\\t%PL]\\n\", \n",
    "        f\"{pb_dir}_joint/{max_patients}_patients.vcf.gz\",\n",
    "        \"-o\", f\"{pb_dir}_tsv/{max_patients}_patients.tsv\"\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's define a list of all patients, and functions for computing GT and PL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save list of patients\n",
    "patients = [p.decode() for p in subprocess.run(\n",
    "    [\"bcftools\", \"query\", \n",
    "         \"--list-samples\", f\"{pb_dir}_joint/{max_patients}_patients.vcf.gz\"], \n",
    "    stdout=subprocess.PIPE).stdout.splitlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace unknown calls with ref-calls, and remove phasing info (1|0 -> 0/1)\n",
    "def gt_type(gt_str):\n",
    "    gt_str = gt_str.replace(\".\",\"0\")\n",
    "    if (gt_str[0] == '0' and gt_str[2] == '1') or (gt_str[0] == '1' and gt_str[2] == '0'): return '0/1'\n",
    "    if gt_str[0] == '0' and gt_str[2] == '0': return '0/0'\n",
    "    if gt_str[0] == '1' and gt_str[2] == '1': return '1/1'\n",
    "    else: return '?/?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse PL data if available; otherwise compute from other fields\n",
    "def get_pl(gt, gq, pl, ac):\n",
    "    if pl == '.': # missing, compute from GT/GQ\n",
    "        if (ac == 0 and gt == '0/0') or (ac == 1 and gt == '0/1') or (ac == 2 and gt == '1/1'): # chosen allele\n",
    "            return 0\n",
    "        else: # not chosen allele\n",
    "            return gq \n",
    "    else: # parse out of PL data\n",
    "        return int(pl.split(',')[ac])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afterwards, we load the TSV into a DataFrame, impute missing values, and perform filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# remove numbered prefix from column names (e.g. \"[1]CHROM\"->\"CHROM\")\n",
    "vcf_df = pd.read_csv(f\"{pb_dir}_tsv/{max_patients}_patients.tsv\", delimiter=\"\\t\", nrows=100000)\n",
    "vcf_df.columns = [col.split(\"]\")[1] for col in vcf_df.columns]\n",
    "\n",
    "# filter out variants which are too homogenous across patients (for more interesting analysis, don't use for real datasets)\n",
    "# a real application may instead have a whitelist/mask of genomic regions or variants of interest\n",
    "min_freq = 0.05\n",
    "max_freq = 0.95\n",
    "gt_is_empty = pd.concat([vcf_df[f'{p}:GT'] == './.' for p in patients], axis=1)\n",
    "gt_is_ref = pd.concat([vcf_df[f'{p}:GT'] == '0/0' for p in patients], axis=1)\n",
    "n_valid = max_patients - (gt_is_empty | gt_is_ref).sum(axis=1)\n",
    "in_freq_range = [x/max_patients < max_freq and x/max_patients > min_freq for x in n_valid]\n",
    "vcf_df = vcf_df[in_freq_range]\n",
    "print(f'selected variants: {len(vcf_df)}')\n",
    "\n",
    "# impute missing values using median for all numeric fields\n",
    "number_cols = [\"QUAL\", \"DP\", \"AQ\"] + [f\"{patient}:GQ\" for patient in patients] + [f\"{patient}:DP\" for patient in patients]\n",
    "print(f'Missing values in the following columns will be imputed using per-patient medians.\\nThis is required because unlike GVCFs, VCFs do not contain depth and quality information for reference calls.')\n",
    "for nc in number_cols:\n",
    "    print(f'{nc} missing: {len(vcf_df[vcf_df[nc] == \".\"])}')\n",
    "    median = pd.to_numeric(vcf_df.loc[vcf_df[nc] != \".\", nc]).median()\n",
    "    vcf_df.loc[vcf_df[nc]=='.', nc] = median\n",
    "    vcf_df[nc] = pd.to_numeric(vcf_df[nc])\n",
    "\n",
    "# compute phred likelihoods and allele frequencies for each sample\n",
    "for p in patients:\n",
    "    vcf_df[f'{p}:GT'] = vcf_df[f'{p}:GT'].map(gt_type)\n",
    "    vcf_df[f'{p}:PL_0/0'] = vcf_df.apply(lambda row: get_pl(gt = row[f'{p}:GT'], gq = row[f'{p}:GQ'], pl = row[f'{p}:PL'], ac = 0), axis=1)\n",
    "    vcf_df[f'{p}:PL_0/1'] = vcf_df.apply(lambda row: get_pl(gt = row[f'{p}:GT'], gq = row[f'{p}:GQ'], pl = row[f'{p}:PL'], ac = 1), axis=1)\n",
    "    vcf_df[f'{p}:PL_1/1'] = vcf_df.apply(lambda row: get_pl(gt = row[f'{p}:GT'], gq = row[f'{p}:GQ'], pl = row[f'{p}:PL'], ac = 2), axis=1)\n",
    "    \n",
    "    vcf_df[f'{p}:AF_0'] = vcf_df.apply(lambda row: 1 if row[f'{p}:AD'] == '.' else int(row[f'{p}:AD'].split(\",\")[0]) / max(row[f'{p}:DP'], 1) , axis=1)\n",
    "    vcf_df[f'{p}:AF_1'] = vcf_df.apply(lambda row: 0 if row[f'{p}:AD'] == '.' else int(row[f'{p}:AD'].split(\",\")[1]) / max(row[f'{p}:DP'], 1) , axis=1)\n",
    "\n",
    "# filter by depth/quality, and remove complex variants\n",
    "vcf_df = vcf_df[vcf_df['DP'] >= 15]\n",
    "vcf_df = vcf_df[vcf_df['QUAL'] >= 20]\n",
    "vcf_df = vcf_df[vcf_df['TYPE'] != 'OTHER']\n",
    "print(f'passing variants: {len(vcf_df)}')\n",
    "\n",
    "# limit to specified number of variants\n",
    "vcf_df = vcf_df.drop(vcf_df.index[max_variants:])\n",
    "vcf_df.reset_index(inplace=True)\n",
    "print(f'selected variants: {len(vcf_df)}')\n",
    "vcf_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we'll transpose the dataframe so that each row corresponds to a single patient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# define the fields we'll be using in our final dataframe\n",
    "fields = []\n",
    "sample_fields = [\"GT\", \"AF_0\", \"AF_1\", \"PL_0/0\", \"PL_0/1\", \"PL_1/1\", \"DP\"]\n",
    "\n",
    "# create empty dataframe\n",
    "pb_df = pd.DataFrame(columns=[f\"{var_id}:{fld}\" for var_id in range(len(vcf_df)) for fld in fields + sample_fields], index=patients)\n",
    "\n",
    "# fill dataframe\n",
    "for idx, row in vcf_df.iterrows():\n",
    "    for f in fields:\n",
    "        for p in patients:\n",
    "            pb_df.loc[p][f\"{idx}:{f}\"] = row[f]\n",
    "    for f in sample_fields:\n",
    "        for p in patients:\n",
    "            pb_df.loc[p][f\"{idx}:{f}\"] = row[f\"{p}:{f}\"]\n",
    "\n",
    "# new index\n",
    "pb_df.reset_index(inplace=True)\n",
    "pb_df = pb_df.rename(columns = {'index': 'patient_id'})\n",
    "pb_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Merging and Normalization <a id=\"4\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since one row corresponds to a single patient in both our PacBio and FHIR datasets, we can easily append these two datasets side by side.  \n",
    "Note that for real applications, each row's PacBio data must be matched up with the FHIR data for the corresponding patient. Since our FHIR data is synthetic, this matchup doesn't matter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = pd.concat([all_fhir_df, pb_df], axis=1)\n",
    "all_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once all this information is in a single table, we can select the most useful columns and convert/normalize data into a format which is best for ML applications. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for normalization\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "mms = MinMaxScaler()\n",
    "\n",
    "# select useful FHIR columns\n",
    "ml_df = pd.DataFrame()\n",
    "ml_df['alive'] = [1-int(x) for x in all_df['dead']]\n",
    "ml_df['twin+'] = [int(x) for x in all_df['twin+']]\n",
    "ml_df['male'] = [1 if g == 'male' else 0 for g in all_df['gender']]\n",
    "ml_df['age'] = all_df['age']\n",
    "ml_df = pd.merge(\n",
    "    left = ml_df,\n",
    "    right = pd.get_dummies(all_df['married']),\n",
    "    left_index = True,\n",
    "    right_index = True,\n",
    ")\n",
    "ml_df = ml_df.rename(columns={'M': 'married', 'N': 'never_married', 'S': 'single'})\n",
    "ml_df['old_report_age'] = all_df['oldest_report_elapsed']\n",
    "ml_df['new_report_age'] = all_df['newest_report_elapsed']\n",
    "ml_df['old_med_age'] = all_df['oldest_med_elapsed']\n",
    "ml_df['new_med_age'] = all_df['newest_med_elapsed']\n",
    "new_med_code_df = pd.get_dummies(all_df['newest_med_code'])\n",
    "ml_df = pd.merge(\n",
    "    left = ml_df,\n",
    "    right = new_med_code_df.add_prefix(\"new_med_code:\"),\n",
    "    left_index = True,\n",
    "    right_index = True,\n",
    ")\n",
    "\n",
    "# add all variants (ignoring DP, AF, PL for now)\n",
    "for var_idx in range(max_variants):\n",
    "    gt_df = pd.get_dummies(all_df[f'{var_idx}:GT'])\n",
    "    ml_df = pd.merge(\n",
    "        left = ml_df,\n",
    "        right = gt_df.add_prefix(f'{var_idx}:'),\n",
    "        left_index = True,\n",
    "        right_index = True,\n",
    "    )\n",
    "\n",
    "    #ml_df[f'{var_idx}:DP'] = all_df[f'{var_idx}:DP']\n",
    "    \n",
    "# normalize\n",
    "age_cols = ['age', 'old_report_age', 'new_report_age', 'old_med_age', 'new_med_age'] #+ [f'{var_idx}:DP' for var_idx in range(max_variants)]\n",
    "ml_df[age_cols] = mms.fit_transform(ml_df[age_cols])\n",
    "ml_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Clustering Analysis <a id=\"5\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can perform clustering, we must select a reasonable number of clusters for grouping patients. The k-means++ and spectral clustering algorithms require a predetermined number of clusters as inputs. This can be done using the \"elbow method\", where we select the number of clusters after which there are diminishing returns (in terms of cluster inertia) of introducing additional clusters. This appears in a graph of (x = # clusters, y = total inertia) as a bend, or elbow. In our dataset, we find a slight elbow at `n_clusters = 4`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "inertias = []\n",
    "for i in range(1,11):\n",
    "    kmeans = KMeans(n_clusters = i).fit(ml_df)\n",
    "    inertias.append(kmeans.inertia_)\n",
    "\n",
    "plt.plot(range(1,11), inertias, marker='o')\n",
    "plt.title('\"Elbow\" Method')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Inertia')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we demonstrate several clustering methods on our patient dataset.  \n",
    "You can find more information about each clustering method at [`scikit-learn`'s Clustering Page](https://scikit-learn.org/stable/modules/clustering.html#clustering)[<sup>[10]</sup>](#r10)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans, DBSCAN, SpectralClustering\n",
    "\n",
    "# three clustering options\n",
    "n_clusters = 4\n",
    "cluster1 = KMeans(n_clusters=n_clusters, init='k-means++', n_init=5).fit(ml_df)\n",
    "cluster2 = DBSCAN(eps=3, min_samples=3).fit(ml_df)\n",
    "cluster3 = SpectralClustering(n_clusters=n_clusters).fit(ml_df)\n",
    "\n",
    "# break down ML DF into top 2 principal components\n",
    "pca2 = PCA(n_components = 2)\n",
    "pc = pca2.fit_transform(ml_df)\n",
    "pc_df = pd.DataFrame(data = pc, columns = ['pc1', 'pc2'])\n",
    "\n",
    "# plot different clustering methods\n",
    "fig, ax = plt.subplots(1,3, figsize=(15,5))\n",
    "methods = [\"K-Means++\", \"DBSCAN\", \"Spectral\"]\n",
    "clusters = [cluster1, cluster2, cluster3]\n",
    "for x in range(3):\n",
    "    ax[x].set_title(f\"{methods[x]} Clustering\")\n",
    "    ax[x].set_xlabel('Principal Component #1')\n",
    "    ax[x].set_ylabel('Principal Component #2')\n",
    "    ax[x].scatter(pc_df['pc1'], pc_df['pc2'], c=clusters[x].labels_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's evaluate the performance of these clustering methods. More information the metrics used can again be found at [`scikit-learn`'s Clustering Page](https://scikit-learn.org/stable/modules/clustering.html#clustering-performance-evaluation)[<sup>[10]</sup>](#r10)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import davies_bouldin_score, calinski_harabasz_score, silhouette_score\n",
    "from  matplotlib.colors import LinearSegmentedColormap\n",
    "import numpy as np\n",
    "\n",
    "# calculate evaluation metrics\n",
    "metrics = [\"Davies-Bouldin\", \"Calinski-Harabasz\", \"Silhouette\"]\n",
    "high_is_good = [False, True, True]\n",
    "scores = np.zeros((3,3))\n",
    "for x in range(3):\n",
    "    scores[x,0] = davies_bouldin_score(ml_df, clusters[x].labels_)\n",
    "    scores[x,1] = calinski_harabasz_score(ml_df, clusters[x].labels_)\n",
    "    scores[x,2] = silhouette_score(ml_df, clusters[x].labels_, metric='euclidean')\n",
    "\n",
    "# plot colormaps\n",
    "fig, ax = plt.subplots(1,3, figsize=(6,5))\n",
    "for x in range(3):\n",
    "    cmap = LinearSegmentedColormap.from_list('rg',[\"r\", \"y\", \"g\"], N=256) if high_is_good[x] else \\\n",
    "           LinearSegmentedColormap.from_list('gr',[\"g\", \"y\", \"r\"], N=256)\n",
    "    ax[x].matshow(np.expand_dims(scores[:,x], axis=1), cmap=cmap)\n",
    "    ax[x].set_xticks([0])\n",
    "    ax[x].set_xticklabels([metrics[x]])\n",
    "    if x == 0:\n",
    "        ax[x].set_yticklabels([0] + methods)\n",
    "    else:\n",
    "        ax[x].set_yticks([])\n",
    "    for y in range(3):\n",
    "        ax[x].text(0, y, f'{scores[y,x]:2f}', va='center', ha='center')\n",
    "\n",
    "# display\n",
    "ax[0].set_ylabel('Clustering Methods')\n",
    "ax[1].set_xlabel('Metrics')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, here's an agglomerative clustering dendrogram, which can also be used to cluster patients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "\n",
    "plt.figure(figsize=(20,5))\n",
    "linkage_data = linkage(ml_df, method='ward', metric='euclidean')\n",
    "dendrogram(linkage_data)\n",
    "plt.xlabel('Patients')\n",
    "plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## References <a id=\"references\"></a>\n",
    "[1]  <a id=\"r1\"></a> Previous Jupyter Notebook: https://github.com/microsoft/genomicsnotebook/blob/main/sample-notebooks/fhir_long_read.ipynb  \n",
    "[2]  <a id=\"r2\"></a> Microsoft Blog Post:https://techcommunity.microsoft.com/t5/healthcare-and-life-sciences/convert-synthetic-fhir-and-pacbio-vcf-data-to-parquet-and/ba-p/3577038  \n",
    "[3]  <a id=\"r3\"></a> FHIR HL7: http://hl7.org/fhir/index.html  \n",
    "[4] <a id=\"r4\"></a> Parquet File Format: https://spark.apache.org/docs/latest/sql-data-sources-parquet.html  \n",
    "[5] <a id=\"r5\"></a> Python `pandas` library: https://pandas.pydata.org/  \n",
    "[6] <a id=\"r6\"></a> Broad GATK GVCF Explanation: https://github.com/broadinstitute/gatk-docs/blob/master/gatk3-faqs/What_is_a_GVCF_and_how_is_it_different_from_a_'regular'_VCF%3F.md  \n",
    "[7] <a id=\"r7\"></a> `bcftools` Documentation: https://samtools.github.io/bcftools/bcftools.html  \n",
    "[8] <a id=\"r8\"></a> HTSlib/SAMtools/Tabix Homepage: https://www.htslib.org/  \n",
    "[9] <a id=\"r9\"></a> Linkage Disequilibrium: https://en.wikipedia.org/wiki/Linkage_disequilibrium  \n",
    "[10] <a id=\"r10\"></a> Scikit-Learn Clustering: https://scikit-learn.org/stable/modules/clustering.html#clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTICES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "THIS NOTEBOOK JUST PROVIDE A SAMPLE CODES FOR EDUCATIONAL PURPOSES. MICROSOFT DOES NOT CLAIM ANY OWNERSHIP ON THESE CODES AND LIBRARIES. MICROSOFT PROVIDES THIS NOTEBOOK AND SAMPLE USE OF  LIBRARIES ON AN “AS IS” BASIS. DATA OR ANY MATERIAL ON THIS NOTEBOOK. MICROSOFT MAKES NO WARRANTIES, EXPRESS OR IMPLIED, GUARANTEES OR CONDITIONS WITH RESPECT TO YOUR USE OF THIS NOTEBOOK. TO THE EXTENT PERMITTED UNDER YOUR LOCAL LAW, MICROSOFT DISCLAIMS ALL LIABILITY FOR ANY DAMAGES OR LOSSES, INCLUDING DIRECT, CONSEQUENTIAL, SPECIAL, INDIRECT, INCIDENTAL OR PUNITIVE, RESULTING FROM YOUR USE OF THIS NOTEBOOK.\n",
    "\n",
    "#### Notebook prepared by [Tim Dunn](https://github.com/TimD1)- Research Intern- Microsoft Biomedical Platforms and Genomics"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python38-azureml"
  },
  "kernelspec": {
   "display_name": "Python 3.8 - AzureML",
   "language": "python",
   "name": "python38-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
