{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "tags": []
   },
   "source": [
    "# Example Pipeline: Pharmacogenomics Analysis on Breast Cancer Patients\n",
    "### Overview\n",
    "This notebook demonstrates an example end-to-end pipeline which combines (synthetic) FHIR data with PacBio long read sequencing data to perform a basic pharmacogenomic study. A custom Synthea module generates a synthetic cohort of breast cancer patients that have been split into two treatment groups (patients take either Epirubicin or Doxorubicin). This module models the fact that certain patients will respond better to one medication than the other, depending upon the presence/absence of a key variant. After merging FHIR and genomic data through a realistic and scalable pipeline, we perform basic statistical tests to discover the optimal treatment method for a given patient, based on their genetic profile. Please note that this is a contrived example for pipeline demonstration purposes only; a real pharmacogenomics application would require significantly more in-depth analyses.\n",
    " \n",
    "**[Section 0](#0)** covers setting up a JupyterLab instance on an Azure Confidential Compute VM.  \n",
    "**[Section 1](#1)** covers variant calling PacBio data using Cromwell on Azure.  \n",
    "**[Section 2](#2)** covers generating FHIR data for a synthetic cohort of breast cancer patients using Synthea.  \n",
    "**[Section 3](#3)** covers FHIR server deployment and configuration.  \n",
    "**[Section 4](#4)** covers uploading Synthea data to the FHIR server.  \n",
    "**[Section 5](#5)** covers deploying a custom \"Sync Agent\" to download FHIR data in parquet format.  \n",
    "**[Section 6](#6)** covers parsing and converting parquet FHIR data into a pandas DataFrame.  \n",
    "**[Section 7](#7)** covers merging individual patient VCFs into a single joint VCF.  \n",
    "**[Section 8](#8)** covers converting the joint VCF into a pandas Dataframe.  \n",
    "**[Section 9](#9)** covers merging the PacBio and FHIR data in a realistic manner.  \n",
    "**[Section 10](#10)** covers basic pharmacogenomics analysis of the merged data.  \n",
    "**[References](#references)**\n",
    "\n",
    "\n",
    "***Disclaimer:** We are providing an example architectural design to illustrate how Microsoft tools can be utilized to connect the pieces together (data + interoperability + secure cloud + AI tools), enabling researchers to conduct research analyzing genomics and clinical data. We are not providing or recommending specific instructions for how investigators should conduct their research with this notebook – we will leave that to the professionals!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Create a Confidential VM<a id=\"0\"></a> \n",
    "First, create a confidential VM following this post:  \n",
    "https://docs.microsoft.com/en-us/azure/confidential-computing/quick-create-confidential-vm-portal-amd\n",
    "\n",
    "When you deploy the instance, it should prompt you with an alert to download the private key. Do this! Next, note the \"Public IP address\" shown under \"Essentials\" when you select your deployed VM's \"Overview\" in the left-pane menu.\n",
    "\n",
    "Using this, you will be able to SSH into the machine from your Terminal. By default, however, the confidential VMs are highly secure and don't accept any connections. We need to first allow connections for our SSH session and JupyterLab server.\n",
    "\n",
    "**Add SSH in:** Virtual machine -> left pane \"Networking\" -> \"Inbound port rules\" -> click \"Add inbound port rule\" -> Allow SSH on port 22 from your IP  \n",
    "**Add JupyterLab in:** Virtual machine -> left pane \"Networking\" -> \"Inbound port rules\" -> click \"Add inbound port rule\" -> Allow Any protocol on port 8080 from your IP  \n",
    "**Add JupyterLab out:** Virtual machine -> left pane \"Networking\" -> \"Outbound port rules\" -> click \"Add outbound port rule\" -> Allow Any protocol on port 8080 to your IP\n",
    "\n",
    "Restart the VM by clicking left pane \"Overview\" then top pane \"Restart\", and you should be able to connect over SSH.\n",
    "```\n",
    "ssh -i <private_key.pem> azureuser@<ip_address>\n",
    "```\n",
    "Then, run the following commands in your Terminal to install up-to-date versions of Python3, pip, virtualenv, and JupyterLab on the confidential VM.\n",
    "```\n",
    "sudo apt update\n",
    "sudo apt upgrade\n",
    "sudo apt install python3-pip python3-dev\n",
    "sudo -H pip3 install --upgrade pip\n",
    "\n",
    "sudo -H pip3 install virtualenv\n",
    "mkdir lab && cd lab\n",
    "virtualenv jupyterenv\n",
    "source jupyterenv/bin/activate\n",
    "\n",
    "pip install jupyterlab pandas pyarrow plotly statsmodels\n",
    "```\n",
    "At this point, you can start the service with the command shown below. Launching the server will also print the required access token to `stdout`. `--no-browser` specifies to not launch a browser on the confidential VM.\n",
    "\n",
    "**Note:** Don't use `--ip 0.0.0.0`, as some tutorials recommend. This will allow anyone to access the JupyterLab server at its public URL over HTTP, compromising security.\n",
    "```\n",
    "jupyter lab --no-browser --port 8080\n",
    "```\n",
    "Using a new Terminal, we need to set up SSH tunneling, which will bind port 8080 on the confidential VM to port 8080 on your local machine.\n",
    "```\n",
    "ssh -i <private_key.pem> -N -L localhost:8080:localhost:8080 azureuser@<ip_address>\n",
    "```\n",
    "Now, you can open JupyterLab in your local browser by navigating to `localhost:8080`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Variant Calling with 'Cromwell on Azure'<a id=\"1\"></a>\n",
    "\n",
    "Please deploy Cromwell on Azure from a non-confidential VM, preferably an Azure ML VM. Deployment from a confidential VM seems to fail during authentication with Microsoft servers.\n",
    "\n",
    "### 1.1 Deploy Cromwell on Azure\n",
    "Cromwell is a workflow management system for scientific workflows, particularly genomics analysis. It was originally developed by the Broad Institute, is included in GATK's best practices pipeline, and runs on Microsoft Azure. First, we'll download the latest release of Cromwell on Azure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://github.com/microsoft/CromwellOnAzure/releases/download/3.0.0/deploy-cromwell-on-azure-linux"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell to install Azure CLI `az`. This should already be installed on AzureML VMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "sudo apt-get update\n",
    "sudo apt-get install -y ca-certificates curl apt-transport-https lsb-release gnupg\n",
    "curl -sL https://packages.microsoft.com/keys/microsoft.asc | gpg --dearmor | \\\n",
    "        sudo tee /etc/apt/trusted.gpg.d/microsoft.gpg > /dev/null\n",
    "AZ_REPO=$(lsb_release -cs)\n",
    "echo \"deb [arch=amd64] https://packages.microsoft.com.repos.azure-cli/ $AZ_REPO main\" | \\\n",
    "        sudo tee /etc/apt/sources.list.d/azure-cli.list\n",
    "sudo apt-get update # fails on the new source we added, but next step succeeds...\n",
    "sudo apt-get install -y azure-cli"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can log in with the proper permissions to modify our resource group and deploy Cromwell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!az login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following command will deploy the necessary storage accounts and compute instances within the specified resource group under the prefix `cvm-coa`.  \n",
    "You can find more information about deploying Cromwell on Azure [here](https://github.com/microsoft/CromwellOnAzure#Deploy-your-instance-of-Cromwell-on-Azure). First, add permission to run the executable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!chmod +x ./deploy-cromwell-on-azure-linux"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** The Cromwell deployment appears to loop, printing the same output repeatedly. It will slowly get farther along in the deployment each time, taking several hours in total. Please be patient, and it will eventually succeed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!./deploy-cromwell-on-azure-linux \\\n",
    "    --SubscriptionId '<########-####-####-####-############>'\n",
    "    --ResourceGroupName '<resource_group_name>' \\\n",
    "    --RegionName eastus \\\n",
    "    --MainIdentifierPrefix '<prefix>'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Configure Input Data\n",
    "\n",
    "We have set up the Github repository https://github.com/timd1/pb-human-wgs-workflow-wdl to run an example variant calling workflow using a small subset of a public dataset: **chromosome 22 of HG002**. This is for demonstration purposes only; you may wish to run a similar pipeline with your own input BAM files and reference FASTA. The following tutorials may also be helpful for getting started: [Cromwell on Azure](https://github.com/microsoft/CromwellOnAzure/blob/main/docs/managing-your-workflow.md) and [PacBio human WGS](https://github.com/PacificBiosciences/pb-human-wgs-workflow-wdl/blob/main/Getting%20Started.md).\n",
    "\n",
    "First, we'll download the reference data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz\n",
    "!wget https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai\n",
    "!gunzip GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz\n",
    "!wget https://raw.githubusercontent.com/TimD1/pb-human-wgs-workflow-wdl/main/chr_lengths.tsv\n",
    "!wget https://raw.githubusercontent.com/PacificBiosciences/pbsv/master/annotations/human_GRCh38_no_alt_analysis_set.trf.bed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll download an HG002 BAM from NCBI. Since this file is 65GB, it may take a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/data/AshkenazimTrio/HG002_NA24385_son/PacBio_SequelII_CCS_11kb/HG002_GRCh38/HG002_GRCh38.haplotag.10x.bam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we'll install `samtools`, which we can use to extract only reads aligning to `chr22`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://github.com/samtools/samtools/releases/download/1.12/samtools-1.12.tar.bz2\n",
    "!tar -xvf samtools-1.12.tar.bz2\n",
    "%cd samtools-1.12\n",
    "!sudo apt-get install -y libncurses-dev libbz2-dev liblzma-dev\n",
    "!./configure\n",
    "!make\n",
    "!sudo make install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "subprocess.run([\"samtools\", \"view\",\n",
    "    \"-h\", \"-b\",\n",
    "    \"-o\", \"HG002_GRCh38.chr22.bam\",\n",
    "    \"HG002_GRCh38.haplotag.10x.bam\",\n",
    "    \"chr22\"\n",
    "]);\n",
    "subprocess.run([\"samtools\", \"index\", \"HG002_GRCh38.chr22.bam\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Triggering the Cromwell Workflow\n",
    "All six files (reference and index FNA/FAI, aligned reads and index BAM/BAI, chromosome lengths TSV, and tandem repeat regions BED) from the previous step should be moved to the Cromwell storage account's `inputs` folder, under the subdirectory `HG2chr22`. You can do this by temporarily mounting the container to this AzureML compute instance. For example, the reference will be located at `/coa<uniq_id>/inputs/HG2chr22/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna`. Note that if you use different filenames, you will need to clone and modify the Github repository to change filepaths.\n",
    "\n",
    "In Cromwell, a trigger file is used to initiate a Cromwell workflow. In order to start the Cromwell variant calling pipeline, download the following trigger file from Github and place it in the `workflows/new` directory of the Cromwell storage account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/TimD1/pb-human-wgs-workflow-wdl/main/examples/AshkenazimTrio/sample/trigger_files/sample.AshkenaziSon.trigger.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generating Synthetic FHIR Data with Synthea<a id=\"2\"></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "At this point, we can start working from the confidential VM deployed in Section 0.\n",
    "\n",
    "Synthea[<sup>[1]</sup>](#r1) is a tool for outputting synthetic, realistic (but not real), patient data and associated health records in a variety of formats. This notebook deals with records in FHIR, or \"Fast Healthcare Interoperability Resources\", format[<sup>[2]</sup>](#r2).  \n",
    "\n",
    "### 2.1 Build Synthea\n",
    "In order to set up Synthea[<sup>[1]</sup>](#r1), first reinstall JDK (otherwise `javadoc` fails during Gradle build)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "!sudo apt-get install -y default-jdk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the latest stable version of the Synthea[<sup>[1]</sup>](#r1) git repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/synthetichealth/synthea.git -b v3.0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Move to `synthea/` prior to building/running Synthea[<sup>[1]</sup>](#r1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd synthea"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build Synthea[<sup>[1]</sup>](#r1) using Gradle 7.0.2 and Java 11.0.15. On Azure `DS11_V2`, this takes anywhere from 2 minutes to 2.5 hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!./gradlew build test check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "Verify that the build/installation has succeeded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!./run_synthea -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Generate Breast Cancer Cohort\n",
    "Add the attached `simple_breast_cancer.json` module to the `synthea/src/main/resources/modules` directory. This is a basic custom module we designed which generates a cohort of breast cancer patients that have been split into two treatment groups (patients take either Epirubicin or Doxorubicin).\n",
    "\n",
    "Optionally, you can delete all other modules from this directory, which will cause Synthea to ONLY model our simplistic breast cancer module. They will now be unrealistic patients (not suffering from any other afflictions), but it will reduce the volume of data that has to be processed by the server or parsed by the downstream pipeline.  \n",
    "**Note:** you must leave the `lookup_tables` subfolder in the `modules` directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "Generate a test dataset of 50 patients (Synthea[<sup>[1]</sup>](#r1) only counts `Alive` patients, so there will likely be considerably more)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "synthea_dir = \"/home/azureuser/notebook/synthea-output\"\n",
    "subprocess.run([\"./run_synthea\",\n",
    "    \"-g\", \"F\", \"-a\", \"30-90\",\n",
    "    \"-p\", \"50\",\n",
    "    f'--exporter.baseDirectory={synthea_dir}'\n",
    "]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# after running Synthea, go back to the home directory\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. FHIR Server Deployment and Configuration <a id=\"3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Create an \"Azure API for FHIR\"[<sup>[3]</sup>](#r3) instance\n",
    "\n",
    "**3.1.1) Identify Resource Group and Subscription**\n",
    "- Navigate to your desired Azure \"Resource Group\", note the name -> \"Overview\" -> copy \"Subscription ID\"\n",
    "- Set `resource_group` and `sub_id` in [Section 4.1](#globals)\n",
    "\n",
    "**3.1.2) Create an \"Azure API for FHIR\"[<sup>[3]</sup>](#r3) instance**, named `<fhir_server>`\n",
    "- Navigate to `https://<fhir_server>.azurehealthcareapis.com/metadata` and verify a \"Capability Statement\" is retrieved.  \n",
    "That means the FHIR server[<sup>[3]</sup>](#r3) is running.\n",
    "- Set `fhir_server` in [Section 4.1](#globals)\n",
    "- Use RBAC[<sup>[6]</sup>](#r6): `<fhir_server>` left pane \"Identity\" -> \"On\" -> \"Save\" -> \"Yes\"\n",
    "\n",
    "### 3.2 Give this JupyterNB access\n",
    "**3.2.1) Register an App** with permission to read/write data to the FHIR server[<sup>[3]</sup>](#r3) (this notebook will be that \"app\" and use those permissions)\n",
    "- Create App: \"Azure Active Directory\"[<sup>[4]</sup>](#r4) -> left pane \"App Registrations -> top bar \"New Registration\" -> name `<fhir_app>` -> \"Register\"\n",
    "- Navigate to App: \"Azure Active Directory\"[<sup>[4]</sup>](#r4) -> left pane \"App Registrations\" -> select `<fhir_app>` -> left pane \"Overview\"\n",
    "- Copy the \"Application (client) ID\" and \"Directory (tenant) ID\", then set `client_id` and `tenant_id` in [Section 4.1](#globals)\n",
    "- *More information on app registration:* [[5]](#r5)\n",
    "\n",
    "**3.2.2) Create Client Secret** for this notebook to prove that it is the \"app\", or client <a id=\"secret\"></a>\n",
    "- Navigate to App: \"Azure Active Directory\"[<sup>[4]</sup>](#r4) -> left pane \"App Registrations\" -> select `<fhir_app>`\n",
    "- Create Secret: left pane \"Certificates & Secrets\" -> \"+ New Client Secret\" -> name `<jnb_secret>` -> Add\n",
    "- Save Secret: copy `<jnb_secret>`'s `Value`, and store as `client_secret` in [Section 4.1](#globals). If you do not copy the `Value` immediately after creation, you will no longer be able to access it, and will need to create a new secret.\n",
    "- *More information on app registration:* [[5]](#r5)\n",
    "\n",
    "**3.2.3) Add Permissions** for this notebook to POST/GET data from the FHIR server[<sup>[3]</sup>](#r3)\n",
    "- Navigate to \"Azure API for FHIR\" server[<sup>[3]</sup>](#r3) named `<fhir_server>`\n",
    "- Select Role: left pane \"Access Control (IAM)\" -> top bar \"+ Add\" -> \"Add role assignment\" -> Role=\"FHIR Data Contributor\"\n",
    "- Select Members: middle tab \"Members\" -> \"Assign access to: User...\" -> \"+ Select members\" -> search `<fhir_app>` (created in step 1) -> \"Select\" -> Review & Assign\n",
    "- *More information on Azure Role-Based Access Control:* [[6]](#r6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Upload Synthea Data to the FHIR Server <a id=\"4\"></a>\n",
    "The script below is based on an auto-generated Postman[<sup>[11]</sup>](#r11) template.  \n",
    "Postman[<sup>[11]</sup>](#r11) is a platform for using REST APIs, and there's a tutorial for using it with FHIR here: [[12]](#r12).\n",
    "\n",
    "### 4.1. Configuration<a id=\"globals\"></a>\n",
    "\n",
    "First, set global variables necessary for querying the FHIR API "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resource_group = \"<resource_group_name>\"\n",
    "sub_id = \"<########-####-####-####-############>\"\n",
    "\n",
    "fhir_server = \"<server_name>\"\n",
    "fhir_url = f\"https://{fhir_server}.azurehealthcareapis.com\"\n",
    "\n",
    "tenant_id = \"<########-####-####-####-############>\"\n",
    "client_id = \"<########-####-####-####-############>\"\n",
    "client_secret = \"<client_secret>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, json\n",
    "from glob import glob\n",
    "from urllib.parse import urlencode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Request an access token, using the [previously-generated client secret](#secret). You can find more information on Azure AD Access Tokens [here](https://docs.microsoft.com/en-us/azure/healthcare-apis/azure-api-for-fhir/azure-active-directory-identity-configuration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set request parameters\n",
    "token_url = f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\"\n",
    "payload = {\n",
    "    'grant_type': 'Client_Credentials',\n",
    "    'client_id': client_id,\n",
    "    'client_secret': client_secret,\n",
    "    'resource': fhir_url\n",
    "}\n",
    "headers = {\n",
    "    'Content-Type': 'application/x-www-form-urlencoded'\n",
    "}\n",
    "\n",
    "# request token from server\n",
    "response = requests.request(\"POST\", token_url, headers=headers, data=urlencode(payload))\n",
    "content = json.loads(response.content)\n",
    "\n",
    "# save token from response\n",
    "if response.status_code == 200:\n",
    "    print(f\"{content['token_type']} access token retrieved for {content['resource']} successfully.\")\n",
    "    bearer_token = content[\"access_token\"]\n",
    "else:\n",
    "    print(f\"ERROR: unexpected status code {response.status_code}.\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Upload FHIR Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prior to running the following script, I have found it useful to increase the provisioned throughput of the FHIR server from 400 to 4000 RU/s. This prevents any rate limiting which can lead to dropped records. After the data has been transferred, I drop the throughput back to 400 RU/s. The setting is available on the left tab of the FHIR server instance, under \"Database\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "# add all hospital/practitioner information first\n",
    "for filename in glob(f\"{synthea_dir}/fhir/*Information*.json\"): # (hospital|practitioner)Information<###>\n",
    "    print(f\"Parsing file '{filename}'...\")\n",
    "    json_file = open(filename, 'r')\n",
    "    json_obj = json.load(json_file)['entry']\n",
    "    for resource in json_obj:\n",
    "    \n",
    "        # craft request to FHIR database using REST API\n",
    "        payload = json.dumps(resource[\"resource\"])\n",
    "        headers = {\n",
    "          'Authorization': f'Bearer {bearer_token}',\n",
    "          'Content-Type': 'application/json'\n",
    "        }\n",
    "\n",
    "        # send request\n",
    "        url = f\"{fhir_url}/{resource['resource']['resourceType']}\"\n",
    "        response = requests.request(\"POST\", url, headers=headers, data=payload)\n",
    "        \n",
    "        # verify success\n",
    "        if response.status_code >= 200 and response.status_code < 300:\n",
    "            pass\n",
    "            #print(f\"{resource['resource']['resourceType']} added successfully.\")\n",
    "        else:\n",
    "            print(f\"ERROR: unexpected status code {response.status_code}.\")\n",
    "            print(response.text)\n",
    "            break\n",
    "\n",
    "# parse all Synthea-generated JSON data\n",
    "for filename in glob(f\"{synthea_dir}/fhir/*_*_*.json\"): # <firstname>_<lastname>_<id>\n",
    "    print(f\"Parsing file '{filename}'...\")\n",
    "    json_file = open(filename, 'r')\n",
    "    json_obj = json.load(json_file)['entry']\n",
    "    for resource in json_obj:\n",
    "    \n",
    "        # craft request to FHIR database using REST API\n",
    "        payload = json.dumps(resource[\"resource\"])\n",
    "        headers = {\n",
    "          'Authorization': f'Bearer {bearer_token}',\n",
    "          'Content-Type': 'application/json'\n",
    "        }\n",
    "\n",
    "        # send request\n",
    "        url = f\"{fhir_url}/{resource['resource']['resourceType']}\"\n",
    "        response = requests.request(\"POST\", url, headers=headers, data=payload)\n",
    "        \n",
    "        # verify success\n",
    "        if response.status_code >= 200 and response.status_code < 300:\n",
    "            pass\n",
    "            #print(f\"{resource['resource']['resourceType']} added successfully.\")\n",
    "        else:\n",
    "            print(f\"ERROR: unexpected status code {response.status_code}.\")\n",
    "            print(response.text)\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following script can be used to delete all FHIR data from the server (ONLY for full database reset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resource_list = [ \"AllergyIntolerance\", \"Encounter\", \"Observation\", \"CarePlan\", \"ExplanationOfBenefit\", \n",
    "#                  \"Organization\", \"CareTeam\", \"ImagingStudy\", \"Patient\", \"Claim\", \"Immunization\",\n",
    "#                  \"Practitioner\", \"Condition\", \"Location\", \"PractitionerRole\", \"Device\", \"Medication\", \n",
    "#                  \"Procedure\", \"DiagnosticReport\", \"MedicationAdministration\", \"Provenance\",\n",
    "#                  \"DocumentReference\", \"MedicationRequest\", \"SupplyDelivery\"]\n",
    "\n",
    "# # delete all resource types\n",
    "# for resource_type in resource_list:\n",
    "#     # looping is required, since GET will only fetch first 10 items\n",
    "#     print(f\"Deleting {resource_type}s...\", end=\"\")\n",
    "#     while True:\n",
    "#         # query for list of all resources\n",
    "#         url = f\"{fhir_url}/{resource_type}\"\n",
    "#         headers = {'Authorization': f'Bearer {bearer_token}'}\n",
    "#         response = requests.request(\"GET\", url, headers=headers, data={})\n",
    "#         fhir_data = json.loads(response.content)\n",
    "\n",
    "#         # delete each resource_type (in chunk of 10)\n",
    "#         try:\n",
    "#             for resource in fhir_data[\"entry\"]:\n",
    "#                 url = f\"{fhir_url}/{resource_type}/{resource['resource']['id']}\"\n",
    "#                 response = requests.request(\"DELETE\", url, headers=headers, data={})\n",
    "#                 if response.status_code >= 200 and response.status_code < 300:\n",
    "#                     # print(f\"{resource_type} {resource['resource']['id']} deleted successfully.\")\n",
    "#                     pass\n",
    "#                 else:\n",
    "#                     print(f\"ERROR: unexpected status code {response.status_code}.\")\n",
    "#                     print(response.text)\n",
    "#         except KeyError:\n",
    "#             print(\"done!\")\n",
    "#             break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Deploying the 'FHIR to Synapse Sync Agent' <a id=\"5\"></a>\n",
    "This notebook section follows the \"FHIR to Synapse Sync Agent\" tutorial provided Microsoft's \"FHIR Analytics Pipelines\" Github repository[<sup>[13]</sup>](#r13).\n",
    "\n",
    "### 5.1. Deploy the \"FHIR to Synapse Sync Agent\" \n",
    "First, deploy the custom Azure template provided by the \"FHIR to Synapse Sync Agent\" tutorial[<sup>[13]</sup>](#r13).\n",
    "- Navigate to the Github repo by clicking [this link](https://github.com/microsoft/FHIR-Analytics-Pipelines/blob/main/FhirToDataLake/docs/Deployment.md).\n",
    "- Scroll down to \"Deployment\", then \"1. Deploy the Pipeline\", and click the blue button \"Deploy to Azure\"\n",
    "- App Name `<sync_agent>` -> set FHIR URL -> Authentication `true` -> Container Name `fhir` -> \"Review + Create\" -> \"Create\"\n",
    "- Note: DO NOT have any dashes or underscores in the container name, or else it will fail silently\n",
    "\n",
    "Then, add permissions for Function App `<sync_agent>` to read FHIR data\n",
    "- `<sync agent>` left pane \"Identity\" ->  \"Azure role assignments\" -> \"+Add role assignment\" -> \"Resource Group\" -> `<resource_group>` -> \"FHIR Data Contributor\"\n",
    "\n",
    "\n",
    "### 5.2. Download FHIR Data \n",
    "First, wait for the sync agent to complete a job, transferring records. Jobs are scheduled to run every five minutes, and this first job may take 10-15 minutes to complete.\n",
    "\n",
    "Afterwards, mount the parquet data on the running AzureML[<sup>[0]</sup>](#r0) machine.\n",
    "- Add Datastore: `<azure_ml>` left pane \"Datastores\" -> \"+ New datastore\" -> name `fhir` -> \"Azure Blob Storage\" -> `<sync_agent_storage>` -> `fhir` -> `<keys from next step>` -> \"Create\"\n",
    "- Get Storage keys: `<sync_agent_storage>` left pane \"Access keys\" -> \"Show\" -> \"Copy to clipboard\"\n",
    "- Mount Datastore: `<azure_ml>` left pane \"Compute\" -> `<azure_ml>` instance -> top bar \"Data (preview)\" -> \"Mount\" -> \"Azure Storage\" -> select `fhir` -> name `fhir`\n",
    "\n",
    "If using a confidential VM, you should mount the container using [blobfuse](https://docs.microsoft.com/en-us/azure/storage/blobs/storage-how-to-mount-container-linux):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://packages.microsoft.com/config/ubuntu/20.04/packages-microsoft-prod.deb\n",
    "!sudo dpkg -i packages-microsoft-prod.deb\n",
    "!sudo apt-get update\n",
    "!sudo apt install blobfuse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mount FHIR data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fhir_storage=\"<storage_account_name>\"\n",
    "fhir_key = \"<storage_account_key>\"\n",
    "with open(\"fhir_blobfuse.cfg\", \"w\") as fhir_blob_cfg:\n",
    "    fhir_blob_cfg.write(f\"accountName {fhir_storage}\\n\")\n",
    "    fhir_blob_cfg.write(f\"accountKey {fhir_key}\\n\")\n",
    "    fhir_blob_cfg.write(f\"containerName fhir\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo mkdir -p /mnt/cvm_fhir\n",
    "!sudo chown azureuser /mnt/cvm_fhir\n",
    "!chmod 600 fhir_blobfuse.cfg\n",
    "!mkdir -p ~/cvm_fhir\n",
    "!blobfuse ~/cvm_fhir \\\n",
    "    --tmp-path=/mnt/cvm_fhir  \\\n",
    "    --config-file=fhir_blobfuse.cfg \\\n",
    "    -o attr_timeout=240 \\\n",
    "    -o entry_timeout=240 \\\n",
    "    -o negative_timeout=120"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mount PacBio Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pb_storage=\"<storage_account_name>\"\n",
    "pb_key = \"<storage_account_key>\"\n",
    "with open(\"pb_blobfuse.cfg\", \"w\") as pb_blob_cfg:\n",
    "    pb_blob_cfg.write(f\"accountName {pb_storage}\\n\")\n",
    "    pb_blob_cfg.write(f\"accountKey {pb_key}\\n\")\n",
    "    pb_blob_cfg.write(f\"containerName pacbio\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo mkdir -p /mnt/cvm_pb\n",
    "!sudo chown azureuser /mnt/cvm_pb\n",
    "!chmod 600 pb_blobfuse.cfg\n",
    "!mkdir -p ~/cvm_pb\n",
    "!blobfuse ~/cvm_pb \\\n",
    "    --tmp-path=/mnt/cvm_pb  \\\n",
    "    --config-file=pb_blobfuse.cfg \\\n",
    "    -o attr_timeout=240 \\\n",
    "    -o entry_timeout=240 \\\n",
    "    -o negative_timeout=120"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Extract FHIR Data <a id=\"6\"></a>\n",
    "### from `.parquet` to `DataFrame` \n",
    "First, we'll import the necessary libraries and set some useful variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manage imports\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from datetime import datetime\n",
    "import os, json, subprocess\n",
    "\n",
    "# filepaths\n",
    "fhir_dir = \"/home/azureuser/cvm_fhir/result\"\n",
    "date = \"2022/08/26\"\n",
    "pb_dir = \"/home/azureuser/cvm_pb\"\n",
    "\n",
    "# globals\n",
    "max_patients = 100\n",
    "max_variants = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll parse all the FHIR `Patient` data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse patient information from each .parquet file\n",
    "fhir_dfs = []\n",
    "for patient_file in glob(f\"{fhir_dir}/Patient/{date}/**/*.parquet\"):\n",
    "    \n",
    "    # create dataframe for this .parquet file\n",
    "    fhir_df = pd.DataFrame()\n",
    "    orig_fhir_df = pd.read_parquet(patient_file)\n",
    "\n",
    "    # set basic fields\n",
    "    fhir_df[\"name\"]    = [f\"{' '.join(name[0]['prefix']) if name[0]['prefix'] else ''} {' '.join(name[0]['given'])} {name[0]['family']}\" for name in orig_fhir_df[\"name\"]]\n",
    "    fhir_df[\"gender\"]  = orig_fhir_df[\"gender\"]\n",
    "    fhir_df[\"dead\"]    = [dead is not None for dead in orig_fhir_df[\"deceased\"]]\n",
    "    \n",
    "    # set age in years from birthDate\n",
    "    ages = []\n",
    "    for birthdate in orig_fhir_df[\"birthDate\"]:\n",
    "        year, month, day = map(int, birthdate.split(\"-\"))\n",
    "        ages.append((datetime.now()-datetime(year, month, day)).total_seconds() / (60*60*24*365.25))\n",
    "    fhir_df[\"age\"]     = ages\n",
    "\n",
    "    # set \"medical record number\", from a host of other identifiers (social security, etc.)\n",
    "    mrns = []\n",
    "    for identifier_list in orig_fhir_df[\"identifier\"]:\n",
    "        for ident in identifier_list:\n",
    "            if ident['type'] and json.loads(ident['type']['coding'])[0]['code'] == \"MR\":\n",
    "                mrns.append(ident['value'])\n",
    "    fhir_df[\"mrn\"]     = mrns\n",
    "    \n",
    "# merge information\n",
    "    fhir_dfs.append(fhir_df)\n",
    "all_fhir_df = pd.concat(fhir_dfs).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dictionary to map patients to medical record numbers (MRNs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrn_dict = {}\n",
    "for idx, row in all_fhir_df.iterrows():\n",
    "    mrn_dict[ row['mrn'] ] = idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the MRN to patient mapping, we can scan through all `Condition` and `MedicationRequest` records to determine which patients have breast cancer, and if so, which medication they are taking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if patients have breast cancer\n",
    "has_breast_cancer = [False]*len(all_fhir_df)\n",
    "for conds_file in glob(f\"{fhir_dir}/Condition/{date}/**/*.parquet\"):\n",
    "    conds_df = pd.read_parquet(conds_file)\n",
    "    for idx, row in conds_df.iterrows():\n",
    "        code = \"None\" if row[\"code\"] is None else row[\"code\"][\"coding\"][0][\"code\"]\n",
    "        mrn = row[\"subject\"][\"reference\"].split(':')[2] # urn:uuid:mrn\n",
    "        if code == \"254837009\":\n",
    "            has_breast_cancer[ mrn_dict[mrn] ] = True\n",
    "all_fhir_df['has_breast_cancer'] = has_breast_cancer\n",
    "\n",
    "# get patient medications\n",
    "used_epi = [False]*len(all_fhir_df)\n",
    "used_doxo = [False]*len(all_fhir_df)\n",
    "has_variant = [False]*len(all_fhir_df)\n",
    "for meds_file in glob(f\"{fhir_dir}/MedicationRequest/{date}/**/*.parquet\"):\n",
    "    meds_df = pd.read_parquet(meds_file)\n",
    "    for idx, row in meds_df.iterrows():\n",
    "        code = \"None\" if row[\"medication\"][\"codeableConcept\"] is None else row[\"medication\"][\"codeableConcept\"][\"coding\"][0][\"code\"]\n",
    "        mrn = row[\"subject\"][\"reference\"].split(':')[2] # urn:uuid:mrn\n",
    "        if code == \"1732186\": # epirubicin\n",
    "            used_epi[ mrn_dict[mrn] ] = True\n",
    "        elif code == \"1790099\": # doxorubicin\n",
    "            used_doxo[ mrn_dict[mrn] ] = True\n",
    "        elif code in [\"ALT\", \"REF\"]:\n",
    "            has_variant[ mrn_dict[mrn] ] = code\n",
    "all_fhir_df['epirubicin'] = used_epi\n",
    "all_fhir_df['doxorubicin'] = used_doxo\n",
    "all_fhir_df['has_variant'] = has_variant\n",
    "all_fhir_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove patients without breast cancer, and with unset REF/ALT variants (both REF and ALT are True)\n",
    "all_fhir_df = all_fhir_df.loc[all_fhir_df['has_breast_cancer'] & all_fhir_df['has_variant']]\n",
    "\n",
    "# ensure exactly one of (epirubicin, doxorubicin) is set\n",
    "all_fhir_df = all_fhir_df.loc[all_fhir_df['epirubicin'] ^ all_fhir_df['doxorubicin']]\n",
    "\n",
    "# remove unnecessary columns\n",
    "all_fhir_df['has_variant'] = all_fhir_df['has_variant'] == \"ALT\"\n",
    "all_fhir_df.drop(['has_breast_cancer', 'doxorubicin'], axis=1, inplace=True)\n",
    "all_fhir_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity check that variant presence is independent of patient treatment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_fhir_df[['epirubicin', 'has_variant']].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, let's limit the dataset size to the specified maximum number of patients and preview the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_fhir_df = all_fhir_df.drop(all_fhir_df.index[max_patients:])\n",
    "all_fhir_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Merge individual VCFs into a joint VCF <a id=\"7\"></a>\n",
    "First, we need to install `bcftools` in order to process our VCF files. The `bcftools` documentation[<sup>[7]</sup>](#r7) can be found [here](https://samtools.github.io/bcftools/bcftools.html).  \n",
    "\n",
    "Please note that `sudo apt-get install bcftools` is insufficient because the Ubuntu 18.04 package repository contains `bcftools-1.7`, which failed for us on this processing pipeline with an old bug. In the following cell, we install a more recent version, `bcftools-1.15`, from source.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo apt-get -y install liblzma-dev libbz2-dev libcurl4-nss-dev\n",
    "!wget https://github.com/samtools/bcftools/releases/download/1.15.1/bcftools-1.15.1.tar.bz2\n",
    "!tar -xf bcftools-1.15.1.tar.bz2\n",
    "%cd bcftools-1.15.1\n",
    "!sudo make install\n",
    "!bcftools --version\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must also install `tabix`[<sup>[8]</sup>](#r8), a tool which indexes VCF files for faster processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo apt-get install -y tabix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're first going to pre-process our VCF files by splitting up sites with multiple alleles into separate records, each with a single allele. For example,\n",
    "\n",
    "CHROM  | POS  | REF  | ALT \n",
    "-------|------|------|----\n",
    "chr20  | 1232 | A    | T,C\n",
    "\n",
    "will become\n",
    "\n",
    "CHROM  | POS  | REF  | ALT  \n",
    "-------|------|------|----\n",
    "chr20  | 1232 | A    | T  \n",
    "chr20  | 1232 | A    | C\n",
    "\n",
    "This ensures that the number of fields is static for each entry, particularly for the `PL` and `AF` columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# skip VCFs which have already been split/pruned\n",
    "orig_vcfs = set(glob(f\"{pb_dir}/**/*.vcf.gz\", recursive=True)) - \\\n",
    "            set(glob(f\"{pb_dir}/**/*_split.vcf.gz\", recursive=True)) - \\\n",
    "            set(glob(f\"{pb_dir}/**/*_pruned.vcf.gz\", recursive=True))\n",
    "\n",
    "for vcf_fn in orig_vcfs:\n",
    "    print(f\"Splitting multi-alleles in '{vcf_fn}'\")\n",
    "    subprocess.run([\n",
    "        \"bcftools\", \"norm\", \n",
    "        \"--multiallelics\", \"-both\",\n",
    "        \"--output-type\", \"z\",\n",
    "        \"--output\", f\"{vcf_fn[:-7]}_split.vcf.gz\", # remove .vcf.gz\n",
    "        vcf_fn\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we perform LD (linkage disequilibrium) pruning[<sup>[9]</sup>](#r9) in order to remove variants with high covariance. Since we're only going to be looking at a small subset of each patient's variants, we want to make sure that the variants we do investigate are fairly independent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for vcf_fn in glob(f\"{pb_dir}/**/*_split.vcf.gz\", recursive=True):\n",
    "    print(f\"Pruning variants in '{vcf_fn}'\")\n",
    "    subprocess.run([\n",
    "        \"bcftools\", \"+prune\", \n",
    "        \"-m\", \"0.2\",\n",
    "        vcf_fn,\n",
    "        \"--output-type\", \"z\",\n",
    "        \"--output\", f\"{vcf_fn[:-13]}_pruned.vcf.gz\", # remove _split.vcf.gz\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Index all individual VCFs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "for vcf_fn in glob(f\"{pb_dir}/**/*_pruned.vcf.gz\", recursive=True): # no gvcfs\n",
    "    print(f\"Indexing '{vcf_fn}'\")\n",
    "    subprocess.run([\"tabix\", \"-f\", \"-p\", \"vcf\", vcf_fn])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge VCF files into single joint VCF, and index it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "subprocess.run([\"bcftools\", \"merge\"] + \n",
    "        [\"-m\", \"none\"] +\n",
    "        list(glob(f\"{pb_dir}/**/*_pruned.vcf.gz\", recursive=True))[:max_patients] +\n",
    "        [\"-o\", f\"{pb_dir}_joint/{max_patients}_patients.vcf.gz\"]\n",
    ")\n",
    "subprocess.run([\"tabix\", \"-f\", \"-p\", \"vcf\", f\"{pb_dir}_joint/{max_patients}_patients.vcf.gz\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Extract joint VCF <a id=\"8\"></a>\n",
    "### from `.vcf` to `DataFrame`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll use `bcftools query` to convert our VCF to a more general format that works well with existing data science libraries: TSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# convert joint VCF to CSV\n",
    "subprocess.run([\"bcftools\", \"query\", \n",
    "        \"--print-header\",\n",
    "        \"-f\", \"%CHROM\\t%POS\\t%TYPE\\t%REF\\t%ALT\\t%QUAL\\t%FILTER\\t%INFO/DP\\t%INFO/AF\\t%INFO/AQ\\t%INFO/AN\\t%INFO/AC[\\t%GT\\t%AD\\t%DP\\t%GQ\\t%PL]\\n\", \n",
    "        f\"{pb_dir}_joint/{max_patients}_patients.vcf.gz\",\n",
    "        \"-o\", f\"{pb_dir}_tsv/{max_patients}_patients.tsv\"\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's define a list of all patients, and functions for computing GT and PL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save list of patients\n",
    "patients = [p.decode() for p in subprocess.run(\n",
    "    [\"bcftools\", \"query\", \n",
    "         \"--list-samples\", f\"{pb_dir}_joint/{max_patients}_patients.vcf.gz\"], \n",
    "    stdout=subprocess.PIPE).stdout.splitlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace unknown calls with ref-calls, and remove phasing info (1|0 -> 0/1)\n",
    "def gt_type(gt_str):\n",
    "    gt_str = gt_str.replace(\".\",\"0\")\n",
    "    if (gt_str[0] == '0' and gt_str[2] == '1') or (gt_str[0] == '1' and gt_str[2] == '0'): return '0/1'\n",
    "    if gt_str[0] == '0' and gt_str[2] == '0': return '0/0'\n",
    "    if gt_str[0] == '1' and gt_str[2] == '1': return '1/1'\n",
    "    else: return '?/?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse PL data if available; otherwise compute from other fields\n",
    "def get_pl(gt, gq, pl, ac):\n",
    "    if pl == '.': # missing, compute from GT/GQ\n",
    "        if (ac == 0 and gt == '0/0') or (ac == 1 and gt == '0/1') or (ac == 2 and gt == '1/1'): # chosen allele\n",
    "            return 0\n",
    "        else: # not chosen allele\n",
    "            return gq \n",
    "    else: # parse out of PL data\n",
    "        return int(pl.split(',')[ac])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afterwards, we load the TSV into a DataFrame, impute missing values, and perform filtering. Note that the requirement of minimum allele frequency to be between 0.3 and 0.7 is particularly restrictive, and is used to look at variants which are particularly divergent for this cohort."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# remove numbered prefix from column names (e.g. \"[1]CHROM\"->\"CHROM\")\n",
    "vcf_df = pd.read_csv(f\"{pb_dir}_tsv/{max_patients}_patients.tsv\", delimiter=\"\\t\", nrows=1000000)\n",
    "vcf_df.columns = [col.split(\"]\")[1] for col in vcf_df.columns]\n",
    "\n",
    "# filter out variants which are too homogenous across patients (for more interesting analysis, don't use for real datasets)\n",
    "# a real application may instead have a whitelist/mask of genomic regions or variants of interest\n",
    "min_freq = 0.3\n",
    "max_freq = 0.7\n",
    "gt_is_empty = pd.concat([vcf_df[f'{p}:GT'] == './.' for p in patients], axis=1)\n",
    "gt_is_ref = pd.concat([vcf_df[f'{p}:GT'] == '0/0' for p in patients], axis=1)\n",
    "n_valid = max_patients - (gt_is_empty | gt_is_ref).sum(axis=1)\n",
    "vcf_df['AF'] = [x/max_patients for x in n_valid]\n",
    "in_freq_range = [x/max_patients < max_freq and x/max_patients > min_freq for x in n_valid]\n",
    "vcf_df = vcf_df[in_freq_range]\n",
    "\n",
    "# print(f'selected variants: {len(vcf_df)}')\n",
    "\n",
    "# impute missing values using median for all numeric fields\n",
    "number_cols = [\"QUAL\", \"DP\", \"AQ\"] + [f\"{patient}:GQ\" for patient in patients] + [f\"{patient}:DP\" for patient in patients]\n",
    "# print(f'Missing values in the following columns will be imputed using per-patient medians.\\nThis is required because unlike GVCFs, VCFs do not contain depth and quality information for reference calls.')\n",
    "for nc in number_cols:\n",
    "    # print(f'{nc} missing: {len(vcf_df[vcf_df[nc] == \".\"])}')\n",
    "    median = pd.to_numeric(vcf_df.loc[vcf_df[nc] != \".\", nc]).median()\n",
    "    if pd.isna(median) and nc[-2:] == 'GQ': median = 15 # no other values to infer from\n",
    "    vcf_df.loc[vcf_df[nc]=='.', nc] = median\n",
    "    vcf_df[nc] = pd.to_numeric(vcf_df[nc])\n",
    "\n",
    "# compute phred likelihoods and allele frequencies for each sample\n",
    "for p in patients:\n",
    "    vcf_df[f'{p}:GT'] = vcf_df[f'{p}:GT'].map(gt_type)\n",
    "    vcf_df[f'{p}:PL_0/0'] = vcf_df.apply(lambda row: get_pl(gt = row[f'{p}:GT'], gq = row[f'{p}:GQ'], pl = row[f'{p}:PL'], ac = 0), axis=1)\n",
    "    vcf_df[f'{p}:PL_0/1'] = vcf_df.apply(lambda row: get_pl(gt = row[f'{p}:GT'], gq = row[f'{p}:GQ'], pl = row[f'{p}:PL'], ac = 1), axis=1)\n",
    "    vcf_df[f'{p}:PL_1/1'] = vcf_df.apply(lambda row: get_pl(gt = row[f'{p}:GT'], gq = row[f'{p}:GQ'], pl = row[f'{p}:PL'], ac = 2), axis=1)\n",
    "    \n",
    "    vcf_df[f'{p}:AF_0'] = vcf_df.apply(lambda row: 1 if row[f'{p}:AD'] == '.' else int(row[f'{p}:AD'].split(\",\")[0]) / max(row[f'{p}:DP'], 1) , axis=1)\n",
    "    vcf_df[f'{p}:AF_1'] = vcf_df.apply(lambda row: 0 if row[f'{p}:AD'] == '.' else int(row[f'{p}:AD'].split(\",\")[1]) / max(row[f'{p}:DP'], 1) , axis=1)\n",
    "\n",
    "# filter by depth/quality, and remove complex variants\n",
    "vcf_df = vcf_df[vcf_df['DP'] >= 15]\n",
    "vcf_df = vcf_df[vcf_df['QUAL'] >= 20]\n",
    "vcf_df = vcf_df[vcf_df['TYPE'] != 'OTHER']\n",
    "print(f'passing variants: {len(vcf_df)}')\n",
    "\n",
    "# limit to specified number of variants\n",
    "vcf_df = vcf_df.drop(vcf_df.index[max_variants:])\n",
    "vcf_df.reset_index(inplace=True)\n",
    "print(f'selected variants: {len(vcf_df)}')\n",
    "vcf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we'll transpose the dataframe so that each row corresponds to a single patient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# define the fields we'll be using in our final dataframe\n",
    "fields = []\n",
    "sample_fields = [\"GT\", \"AF_0\", \"AF_1\", \"PL_0/0\", \"PL_0/1\", \"PL_1/1\", \"DP\"]\n",
    "\n",
    "# create empty dataframe\n",
    "pb_df = pd.DataFrame(columns=[f\"{var_id}:{fld}\" for var_id in range(len(vcf_df)) for fld in fields + sample_fields], index=patients)\n",
    "\n",
    "# fill dataframe\n",
    "for idx, row in vcf_df.iterrows():\n",
    "    for f in fields:\n",
    "        for p in patients:\n",
    "            pb_df.loc[p][f\"{idx}:{f}\"] = row[f]\n",
    "    for f in sample_fields:\n",
    "        for p in patients:\n",
    "            pb_df.loc[p][f\"{idx}:{f}\"] = row[f\"{p}:{f}\"]\n",
    "\n",
    "# new index\n",
    "pb_df.reset_index(inplace=True)\n",
    "pb_df = pb_df.rename(columns = {'index': 'patient_id'})\n",
    "pb_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Merge PacBio and FHIR Data <a id=\"9\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since one row corresponds to a single patient in both our PacBio and FHIR datasets, we can easily append these two datasets side by side.  \n",
    "\n",
    "Care is taken to sort both datasets to ensure that the PacBio and FHIR datasets agree whether or not there is a variant at a particular position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge PacBio+FHIR data for patients with variant\n",
    "fhir_w_var = all_fhir_df.loc[all_fhir_df['has_variant'] == True]\n",
    "fhir_w_var.reset_index(inplace=True, drop=True)\n",
    "pb_w_var = pb_df.loc[pb_df['1:GT'] != '0/0']\n",
    "pb_w_var.reset_index(inplace=True, drop=True)\n",
    "min_w_var = min(len(fhir_w_var), len(pb_w_var))\n",
    "w_var = pd.concat([fhir_w_var[:min_w_var], pb_w_var[:min_w_var]], axis=1)\n",
    "\n",
    "# merge PacBio+FHIR data for patients without variant\n",
    "fhir_wo_var = all_fhir_df.loc[all_fhir_df['has_variant'] == False]\n",
    "fhir_wo_var.reset_index(inplace=True, drop=True)\n",
    "pb_wo_var = pb_df.loc[pb_df['1:GT'] == '0/0']\n",
    "pb_wo_var.reset_index(inplace=True, drop=True)\n",
    "min_wo_var = min(len(fhir_wo_var), len(pb_wo_var))\n",
    "wo_var = pd.concat([fhir_wo_var[:min_wo_var], pb_wo_var[:min_wo_var]], axis=1)\n",
    "\n",
    "# merge\n",
    "all_df = pd.concat([w_var, wo_var], axis=0)\n",
    "all_df.reset_index(inplace=True, drop=True)\n",
    "all_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Basic Pharmacogenomics Analysis <a id=\"10\"></a>\n",
    "\n",
    "First, we'll extract only the relevant columns from the combined data, and count the number of patients in each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = pd.DataFrame(all_df[['has_variant', 'epirubicin', 'dead']].value_counts()).reset_index()\n",
    "counts.columns = ['has_variant', 'epirubicin', 'dead', 'counts']\n",
    "counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This sankey plot can be used to more easily visualize the grouping of patients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "fig = go.Figure(data=[go.Sankey(\n",
    "    node = dict(\n",
    "      thickness = 5,\n",
    "      line = dict(color = \"green\", width = 0.1),\n",
    "      label = [\"Patients\", \"Variant\", \"No Variant\", \"Doxorubicin - Variant\", \"Doxorubicin - No Variant\", \n",
    "               \"Epirubicin - Variant\", \"Epirubicin - No Variant\", \"Dead\", \"Alive\"],\n",
    "      color = \"blue\"\n",
    "    ),\n",
    "    link = dict(\n",
    "          \n",
    "      # indices correspond to labels\n",
    "      source = [0,   0,  1,  1,  2,  2, 3,  4,  5,  6, 3, 4, 5, 6], \n",
    "      target = [1,   2,  3,  5,  4,  6, 8,  8,  8,  8, 7, 7, 7, 7],\n",
    "      value =  [\n",
    "                counts[counts['has_variant']].sum()['counts'],\n",
    "                counts[~counts['has_variant']].sum()['counts'],\n",
    "                counts[counts['has_variant'] & ~counts['epirubicin']].sum()['counts'],\n",
    "                counts[counts['has_variant'] & counts['epirubicin']].sum()['counts'],\n",
    "                counts[~counts['has_variant'] & ~counts['epirubicin']].sum()['counts'],\n",
    "                counts[~counts['has_variant'] & counts['epirubicin']].sum()['counts'],\n",
    "                counts[counts['has_variant'] & ~counts['epirubicin'] & ~counts['dead']].sum()['counts'],\n",
    "                counts[~counts['has_variant'] & ~counts['epirubicin'] & ~counts['dead']].sum()['counts'],\n",
    "                counts[counts['has_variant'] & counts['epirubicin'] & ~counts['dead']].sum()['counts'],\n",
    "                counts[~counts['has_variant'] & counts['epirubicin'] & ~counts['dead']].sum()['counts'],\n",
    "                counts[counts['has_variant'] & ~counts['epirubicin'] & counts['dead']].sum()['counts'],\n",
    "                counts[~counts['has_variant'] & ~counts['epirubicin'] & counts['dead']].sum()['counts'],\n",
    "                counts[counts['has_variant'] & counts['epirubicin'] & counts['dead']].sum()['counts'],\n",
    "                counts[~counts['has_variant'] & counts['epirubicin'] & counts['dead']].sum()['counts'],\n",
    "               ]\n",
    "  ))])\n",
    " \n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is known that the survival rate of patients with and without the variant is 50%. Let's see if the medication had any effect on survivability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.weightstats import ztest\n",
    "\n",
    "# 3 \"Doxorubicin - Variant\"\n",
    "alive = [False]*counts[counts['has_variant'] & ~counts['epirubicin'] & counts['dead']].sum()['counts'] + \\\n",
    "        [True]*counts[counts['has_variant'] & ~counts['epirubicin'] & ~counts['dead']].sum()['counts']\n",
    "tstat, pval = ztest(alive, value=0.5, alternative='larger')\n",
    "print(f\"There is a {(1-pval)*100:.5f}% chance Doxorubicin improves survivability for patients WITH Variant.\")\n",
    "\n",
    "# 4 \"Doxorubicin - No Variant\"\n",
    "alive = [False]*counts[~counts['has_variant'] & ~counts['epirubicin'] & counts['dead']].sum()['counts'] + \\\n",
    "        [True]*counts[~counts['has_variant'] & ~counts['epirubicin'] & ~counts['dead']].sum()['counts']\n",
    "tstat, pval = ztest(alive, value=0.5, alternative='larger')\n",
    "print(f\"There is a {(1-pval)*100:.5f}% chance Doxorubicin improves survivability for patients WITHOUT Variant.\")\n",
    "\n",
    "# 5 \"Epirubicin - Variant\"\n",
    "alive = [False]*counts[counts['has_variant'] & counts['epirubicin'] & counts['dead']].sum()['counts'] + \\\n",
    "        [True]*counts[counts['has_variant'] & counts['epirubicin'] & ~counts['dead']].sum()['counts']\n",
    "tstat, pval = ztest(alive, value=0.5, alternative='larger')\n",
    "print(f\"There is a {(1-pval)*100:.5f}% chance Epirubicin improves survivability for patients WITH Variant.\")\n",
    "\n",
    "# 6 \"Epirubicin - No Variant\"\n",
    "alive = [False]*counts[~counts['has_variant'] & counts['epirubicin'] & counts['dead']].sum()['counts'] + \\\n",
    "        [True]*counts[~counts['has_variant'] & counts['epirubicin'] & ~counts['dead']].sum()['counts']\n",
    "tstat, pval = ztest(alive, value=0.5, alternative='larger')\n",
    "print(f\"There is a {(1-pval)*100:.5f}% chance Epirubicin improves survivability for patients WITHOUT Variant.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we select a confidence threshold of p=0.01, then only results with greater than 99% confidence are significant.\n",
    "\n",
    "We can conclude from the above results that Doxorubicin should be prescribed for patients without the variant, and epirubicin should be prescribed for patients with the variant, since both of these treatments result in statistically significant improved outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References <a id=\"references\"></a>\n",
    "[0]  <a id=\"r0\"></a> Azure Machine Learning: https://docs.microsoft.com/en-us/azure/machine-learning/  \n",
    "[1]  <a id=\"r1\"></a> Walonoski, Jason, et al. \"Synthea: An approach, method, and software mechanism for generating synthetic patients and the synthetic electronic health care record.\" Journal of the American Medical Informatics Association 25.3 (2018): 230-238. The MITRE Corporation. https://github.com/synthetichealth/synthea  \n",
    "[2]  <a id=\"r2\"></a> FHIR HL7: http://hl7.org/fhir/index.html  \n",
    "[3]  <a id=\"r3\"></a> Azure API for FHIR: https://docs.microsoft.com/en-us/azure/healthcare-apis/fhir/overview  \n",
    "[4] <a id=\"r4\"></a> Azure Active Directory: https://docs.microsoft.com/en-us/azure/active-directory/fundamentals/active-directory-whatis  \n",
    "[5] <a id=\"r5\"></a> Azure App Registration: https://docs.microsoft.com/en-us/azure/healthcare-apis/register-application  \n",
    "[6] <a id=\"r6\"></a> Azure Role Based Access Control (RBAC): https://docs.microsoft.com/en-us/azure/healthcare-apis/azure-api-for-fhir/configure-azure-rbac  \n",
    "[7] <a id=\"r7\"></a> Azure AD Access Tokens: https://docs.microsoft.com/en-us/azure/healthcare-apis/azure-api-for-fhir/azure-active-directory-identity-configuration  \n",
    "[8] <a id=\"r8\"></a> Microsoft `fhir-loader`: https://github.com/microsoft/fhir-loader  \n",
    "[9] <a id=\"r9\"></a> Azure `bulk-import`: https://docs.microsoft.com/en-us/azure/healthcare-apis/fhir/configure-import-data  \n",
    "[10] <a id=\"r10\"></a> Azure Healthcare APIs changelog: https://docs.microsoft.com/en-us/azure/templates/microsoft.healthcareapis/change-log/services  \n",
    "[11] <a id=\"r11\"></a> Postman API Platform: https://www.postman.com/  \n",
    "[12] <a id=\"r12\"></a> Postman FHIR Tutorial: https://docs.microsoft.com/en-us/azure/healthcare-apis/fhir/use-postman  \n",
    "[13] <a id=\"r13\"></a> FHIR to Synapse Sync Agent Tutorial: https://github.com/microsoft/FHIR-Analytics-Pipelines/blob/main/FhirToDataLake/docs/Deployment.md  \n",
    "[14] <a id=\"r14\"></a> `bcftools` Documentation: https://samtools.github.io/bcftools/bcftools.html  \n",
    "[15] <a id=\"r15\"></a> Parquet File Format: https://spark.apache.org/docs/latest/sql-data-sources-parquet.html  \n",
    "[16] <a id=\"r16\"></a> Python `pandas` library: https://pandas.pydata.org/  \n",
    "[17] <a id=\"r17\"></a> Azure Synapse Analytics: https://docs.microsoft.com/en-us/azure/synapse-analytics/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "THIS NOTEBOOK JUST PROVIDE A SAMPLE CODES FOR EDUCATIONAL PURPOSES. MICROSOFT DOES NOT CLAIM ANY OWNERSHIP ON THESE CODES AND LIBRARIES. MICROSOFT PROVIDES THIS NOTEBOOK AND SAMPLE USE OF  LIBRARIES ON AN “AS IS” BASIS. DATA OR ANY MATERIAL ON THIS NOTEBOOK. MICROSOFT MAKES NO WARRANTIES, EXPRESS OR IMPLIED, GUARANTEES OR CONDITIONS WITH RESPECT TO YOUR USE OF THIS NOTEBOOK. TO THE EXTENT PERMITTED UNDER YOUR LOCAL LAW, MICROSOFT DISCLAIMS ALL LIABILITY FOR ANY DAMAGES OR LOSSES, INCLUDING DIRECT, CONSEQUENTIAL, SPECIAL, INDIRECT, INCIDENTAL OR PUNITIVE, RESULTING FROM YOUR USE OF THIS NOTEBOOK.\n",
    "\n",
    "#### Notebook prepared by [Tim Dunn](https://github.com/TimD1)- Research Intern- Microsoft Biomedical Platforms and Genomics"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python38-azureml"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
